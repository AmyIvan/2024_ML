{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "\n",
    "device = torch.device('cuda:6' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 15219 entries, 0 to 15218\n",
      "Data columns (total 17 columns):\n",
      " #   Column      Non-Null Count  Dtype         \n",
      "---  ------      --------------  -----         \n",
      " 0   instant     15216 non-null  float64       \n",
      " 1   dteday      15216 non-null  datetime64[ns]\n",
      " 2   season      15216 non-null  float64       \n",
      " 3   yr          15216 non-null  float64       \n",
      " 4   mnth        15216 non-null  float64       \n",
      " 5   hr          15216 non-null  float64       \n",
      " 6   holiday     15216 non-null  float64       \n",
      " 7   weekday     15216 non-null  float64       \n",
      " 8   workingday  15216 non-null  float64       \n",
      " 9   weathersit  15216 non-null  float64       \n",
      " 10  temp        15216 non-null  float64       \n",
      " 11  atemp       15216 non-null  float64       \n",
      " 12  hum         15216 non-null  float64       \n",
      " 13  windspeed   15216 non-null  float64       \n",
      " 14  casual      15216 non-null  float64       \n",
      " 15  registered  15216 non-null  float64       \n",
      " 16  cnt         15216 non-null  float64       \n",
      "dtypes: datetime64[ns](1), float64(16)\n",
      "memory usage: 2.0 MB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# 读取../data/train_data.csv文件\n",
    "train_df = pd.read_csv('../data/train_data.csv')\n",
    "# val_df = pd.read_csv('../data/test_data.csv')\n",
    "\n",
    "\n",
    "train_df['dteday'] = pd.to_datetime(train_df['dteday'])\n",
    "\n",
    "# # 提取年、月、日，添加为新的整数字段\n",
    "# train_df['year'] = train_df['dteday'].dt.year\n",
    "# train_df['month'] = train_df['dteday'].dt.month\n",
    "# train_df['day'] = train_df['dteday'].dt.day\n",
    "\n",
    "\n",
    "# val_df['dteday'] = pd.to_datetime(val_df['dteday'])\n",
    "\n",
    "# # 提取年、月、日，添加为新的整数字段\n",
    "# val_df['year'] = val_df['dteday'].dt.year\n",
    "# val_df['month'] = val_df['dteday'].dt.month\n",
    "# val_df['day'] = val_df['dteday'].dt.day\n",
    "\n",
    "# 删除date列\n",
    "# train_df.drop('dteday', axis=1, inplace=True)\n",
    "# val_df.drop('dteday', axis=1, inplace=True)\n",
    "\n",
    "\n",
    "# 查看数据集的前几行，并打印数据集的基本信息\n",
    "\n",
    "print(train_df.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>instant</th>\n",
       "      <th>dteday</th>\n",
       "      <th>season</th>\n",
       "      <th>yr</th>\n",
       "      <th>mnth</th>\n",
       "      <th>hr</th>\n",
       "      <th>holiday</th>\n",
       "      <th>weekday</th>\n",
       "      <th>workingday</th>\n",
       "      <th>weathersit</th>\n",
       "      <th>temp</th>\n",
       "      <th>atemp</th>\n",
       "      <th>hum</th>\n",
       "      <th>windspeed</th>\n",
       "      <th>casual</th>\n",
       "      <th>registered</th>\n",
       "      <th>cnt</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.0</td>\n",
       "      <td>2011-01-01</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.24</td>\n",
       "      <td>0.2879</td>\n",
       "      <td>0.81</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>16.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2.0</td>\n",
       "      <td>2011-01-01</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.22</td>\n",
       "      <td>0.2727</td>\n",
       "      <td>0.80</td>\n",
       "      <td>0.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>32.0</td>\n",
       "      <td>40.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3.0</td>\n",
       "      <td>2011-01-01</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.22</td>\n",
       "      <td>0.2727</td>\n",
       "      <td>0.80</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>27.0</td>\n",
       "      <td>32.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4.0</td>\n",
       "      <td>2011-01-01</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.24</td>\n",
       "      <td>0.2879</td>\n",
       "      <td>0.75</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>13.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5.0</td>\n",
       "      <td>2011-01-01</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.24</td>\n",
       "      <td>0.2879</td>\n",
       "      <td>0.75</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   instant     dteday  season   yr  mnth   hr  holiday  weekday  workingday  \\\n",
       "0      1.0 2011-01-01     1.0  0.0   1.0  0.0      0.0      6.0         0.0   \n",
       "1      2.0 2011-01-01     1.0  0.0   1.0  1.0      0.0      6.0         0.0   \n",
       "2      3.0 2011-01-01     1.0  0.0   1.0  2.0      0.0      6.0         0.0   \n",
       "3      4.0 2011-01-01     1.0  0.0   1.0  3.0      0.0      6.0         0.0   \n",
       "4      5.0 2011-01-01     1.0  0.0   1.0  4.0      0.0      6.0         0.0   \n",
       "\n",
       "   weathersit  temp   atemp   hum  windspeed  casual  registered   cnt  \n",
       "0         1.0  0.24  0.2879  0.81        0.0     3.0        13.0  16.0  \n",
       "1         1.0  0.22  0.2727  0.80        0.0     8.0        32.0  40.0  \n",
       "2         1.0  0.22  0.2727  0.80        0.0     5.0        27.0  32.0  \n",
       "3         1.0  0.24  0.2879  0.75        0.0     3.0        10.0  13.0  \n",
       "4         1.0  0.24  0.2879  0.75        0.0     0.0         1.0   1.0  "
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "categorical_covariates = ['season','yr','mnth','hr','holiday','weekday','workingday','weathersit']\n",
    "\n",
    "categorical_covariates_num_embeddings = []\n",
    "for col in categorical_covariates:\n",
    "    categorical_covariates_num_embeddings.append(train_df[col].nunique())\n",
    "    # categorical_covariates_num_embeddings[1].append(val_df[col].nunique())\n",
    "\n",
    "categorical_static = ['instant']\n",
    "\n",
    "categorical_static_num_embeddings = []\n",
    "for col in categorical_static:\n",
    "    train_df[col] = train_df[col].astype('category').cat.codes\n",
    "    categorical_static_num_embeddings.append(train_df[col].nunique())\n",
    "\n",
    "\n",
    "numeric_covariates = ['temp','atemp','hum','windspeed','casual','registered','cnt']\n",
    "target_idx = np.where(np.array(numeric_covariates)=='cnt')[0][0]\n",
    "\n",
    "\n",
    "\n",
    "# ,,,,,,,,,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>instant</th>\n",
       "      <th>dteday</th>\n",
       "      <th>season</th>\n",
       "      <th>yr</th>\n",
       "      <th>mnth</th>\n",
       "      <th>hr</th>\n",
       "      <th>holiday</th>\n",
       "      <th>weekday</th>\n",
       "      <th>workingday</th>\n",
       "      <th>weathersit</th>\n",
       "      <th>temp</th>\n",
       "      <th>atemp</th>\n",
       "      <th>hum</th>\n",
       "      <th>windspeed</th>\n",
       "      <th>casual</th>\n",
       "      <th>registered</th>\n",
       "      <th>cnt</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>2011-01-01</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.24</td>\n",
       "      <td>0.2879</td>\n",
       "      <td>0.81</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>16.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>2011-01-01</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.22</td>\n",
       "      <td>0.2727</td>\n",
       "      <td>0.80</td>\n",
       "      <td>0.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>32.0</td>\n",
       "      <td>40.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>2011-01-01</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.22</td>\n",
       "      <td>0.2727</td>\n",
       "      <td>0.80</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>27.0</td>\n",
       "      <td>32.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>2011-01-01</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.24</td>\n",
       "      <td>0.2879</td>\n",
       "      <td>0.75</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>13.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>2011-01-01</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.24</td>\n",
       "      <td>0.2879</td>\n",
       "      <td>0.75</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   instant     dteday  season   yr  mnth   hr  holiday  weekday  workingday  \\\n",
       "0        0 2011-01-01     1.0  0.0   1.0  0.0      0.0      6.0         0.0   \n",
       "1        1 2011-01-01     1.0  0.0   1.0  1.0      0.0      6.0         0.0   \n",
       "2        2 2011-01-01     1.0  0.0   1.0  2.0      0.0      6.0         0.0   \n",
       "3        3 2011-01-01     1.0  0.0   1.0  3.0      0.0      6.0         0.0   \n",
       "4        4 2011-01-01     1.0  0.0   1.0  4.0      0.0      6.0         0.0   \n",
       "\n",
       "   weathersit  temp   atemp   hum  windspeed  casual  registered   cnt  \n",
       "0         1.0  0.24  0.2879  0.81        0.0     3.0        13.0  16.0  \n",
       "1         1.0  0.22  0.2727  0.80        0.0     8.0        32.0  40.0  \n",
       "2         1.0  0.22  0.2727  0.80        0.0     5.0        27.0  32.0  \n",
       "3         1.0  0.24  0.2879  0.75        0.0     3.0        10.0  13.0  \n",
       "4         1.0  0.24  0.2879  0.75        0.0     0.0         1.0   1.0  "
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def dataframe_to_tensor(series,numeric_covariates,categorical_covariates,target_idx):\n",
    "\n",
    "#     numeric_cov_arr = np.array([series[numeric_covariates].values.tolist()])\n",
    "#     category_cov_arr = np.array([series[categorical_covariates].values.tolist()])\n",
    "#     static_cov_arr = np.array(series[categorical_static].values.tolist())\n",
    "\n",
    "#     x_numeric = torch.tensor(numeric_cov_arr,dtype=torch.float32).transpose(2,1)\n",
    "#     x_numeric = torch.log(x_numeric+1e-5)\n",
    "#     x_category = torch.tensor(category_cov_arr,dtype=torch.long).transpose(2,1)\n",
    "#     x_static = torch.tensor(static_cov_arr,dtype=torch.long)\n",
    "#     y = torch.tensor(numeric_cov_arr[:,target_idx,:],dtype=torch.float32)\n",
    "\n",
    "#     return x_numeric, x_category, y\n",
    "\n",
    "\n",
    "# window_size = 16\n",
    "# forecast_length = 16\n",
    "# num_val = 2\n",
    "\n",
    "# val_max_date = '2012/10/1'\n",
    "# train_max_date = str((pd.to_datetime(val_max_date) - pd.Timedelta(days=window_size*num_val+forecast_length)).date())\n",
    "\n",
    "# train_final = train_df[train_df['dteday']<=train_max_date]\n",
    "# val_final = train_df[(train_df['dteday']>train_max_date)&(train_df['dteday']<=val_max_date)]\n",
    "# train_final = train_df\n",
    "# # val_final = val_df\n",
    "\n",
    "# train_series = train_final.groupby(categorical_static+['family']).agg(list).reset_index()\n",
    "# # val_series = val_final.groupby(categorical_static+['family']).agg(list).reset_index()\n",
    "\n",
    "# x_numeric_train_tensor, x_category_train_tensor, y_train_tensor = dataframe_to_tensor(train_df,numeric_covariates,categorical_covariates,target_idx)\n",
    "\n",
    "# # x_numeric_val_tensor, x_category_val_tensor, y_val_tensor = dataframe_to_tensor(val_df,numeric_covariates,categorical_covariates,target_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dataframe_to_tensor(series,numeric_covariates,categorical_covariates,categorical_static,target_idx):\n",
    "\n",
    "    numeric_cov_arr = np.array(series[numeric_covariates].values.tolist())\n",
    "    category_cov_arr = np.array(series[categorical_covariates].values.tolist())\n",
    "    static_cov_arr = np.array(series[categorical_static].values.tolist())\n",
    "\n",
    "    x_numeric = torch.tensor(numeric_cov_arr,dtype=torch.float32).transpose(2,1)\n",
    "    x_numeric = torch.log(x_numeric+1e-5)\n",
    "    x_category = torch.tensor(category_cov_arr,dtype=torch.long).transpose(2,1)\n",
    "    x_static = torch.tensor(static_cov_arr,dtype=torch.long)\n",
    "    y = torch.tensor(numeric_cov_arr[:,target_idx,:],dtype=torch.float32)\n",
    "\n",
    "    return x_numeric, x_category, x_static, y\n",
    "\n",
    "\n",
    "window_size = 16\n",
    "forecast_length = 16\n",
    "num_val = 2\n",
    "\n",
    "val_max_date = '2012/10/1'\n",
    "train_max_date = str((pd.to_datetime(val_max_date) - pd.Timedelta(days=window_size*num_val+forecast_length)).date())\n",
    "\n",
    "train_final = train_df[train_df['dteday']<=train_max_date]\n",
    "val_final = train_df[(train_df['dteday']>train_max_date)&(train_df['dteday']<=val_max_date)]\n",
    "\n",
    "train_series = train_final.groupby(categorical_static).agg(list).reset_index()\n",
    "val_series = val_final.groupby(categorical_static).agg(list).reset_index()\n",
    "\n",
    "x_numeric_train_tensor, x_category_train_tensor, x_static_train_tensor, y_train_tensor = dataframe_to_tensor(train_series,numeric_covariates,categorical_covariates,categorical_static,target_idx)\n",
    "\n",
    "x_numeric_val_tensor, x_category_val_tensor, x_static_val_tensor, y_val_tensor = dataframe_to_tensor(val_series,numeric_covariates,categorical_covariates,categorical_static,target_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(numeric_covariates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([14083, 1])"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train_tensor.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "def divide_shuffle(df,div_num):\n",
    "    space = df.shape[0]//div_num\n",
    "    division = np.arange(0,df.shape[0],space)\n",
    "    return pd.concat([df.iloc[division[i]:division[i]+space,:].sample(frac=1) for i in range(len(division))])\n",
    "\n",
    "def create_time_blocks(time_length,window_size):\n",
    "    start_idx = np.random.randint(0,window_size-1)\n",
    "    end_idx = time_length-window_size-16-1\n",
    "    time_indices = np.arange(start_idx,end_idx+1,window_size)[:-1]\n",
    "    time_indices = np.append(time_indices,end_idx)\n",
    "    return time_indices\n",
    "\n",
    "def data_loader(x_numeric_tensor, x_category_tensor, x_static_tensor, y_tensor, batch_size, time_shuffle):\n",
    "\n",
    "    num_series = x_numeric_tensor.shape[0]\n",
    "    time_length = x_numeric_tensor.shape[1]\n",
    "    index_pd = pd.DataFrame({'serie_idx':range(num_series)})\n",
    "    index_pd['time_idx'] = [create_time_blocks(time_length,window_size) for n in range(index_pd.shape[0])]\n",
    "    if time_shuffle:\n",
    "        index_pd = index_pd.explode('time_idx')\n",
    "        index_pd = index_pd.sample(frac=1)\n",
    "    else:\n",
    "        index_pd = index_pd.explode('time_idx').sort_values('time_idx')\n",
    "        index_pd = divide_shuffle(index_pd,5)\n",
    "    indices = np.array(index_pd).astype(int)\n",
    "\n",
    "    for batch_idx in np.arange(0,indices.shape[0],batch_size):\n",
    "\n",
    "        cur_indices = indices[batch_idx:batch_idx+batch_size,:]\n",
    "\n",
    "        x_numeric = torch.stack([x_numeric_tensor[n[0],n[1]:n[1]+window_size,:] for n in cur_indices])\n",
    "        x_category = torch.stack([x_category_tensor[n[0],n[1]:n[1]+window_size,:] for n in cur_indices])\n",
    "        x_static = torch.stack([x_static_tensor[n[0],:] for n in cur_indices])\n",
    "        y = torch.stack([y_tensor[n[0],n[1]+window_size:n[1]+window_size+forecast_length] for n in cur_indices])\n",
    "\n",
    "        yield x_numeric.to(device), x_category.to(device), x_static.to(device), y.to(device)\n",
    "\n",
    "def val_loader(x_numeric_tensor, x_category_tensor, x_static_tensor, y_tensor, batch_size, num_val):\n",
    "\n",
    "    num_time_series = x_numeric_tensor.shape[0]\n",
    "\n",
    "    for i in range(num_val):\n",
    "\n",
    "        for batch_idx in np.arange(0,num_time_series,batch_size):\n",
    "\n",
    "            x_numeric = x_numeric_tensor[batch_idx:batch_idx+batch_size,window_size*i:window_size*(i+1),:]\n",
    "            x_category = x_category_tensor[batch_idx:batch_idx+batch_size,window_size*i:window_size*(i+1),:]\n",
    "            x_static = x_static_tensor[batch_idx:batch_idx+batch_size]\n",
    "            y_val = y_tensor[batch_idx:batch_idx+batch_size,window_size*(i+1):window_size*(i+1)+forecast_length]\n",
    "\n",
    "            yield x_numeric.to(device), x_category.to(device), x_static.to(device), y_val.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "class transformer_block(nn.Module):\n",
    "\n",
    "    def __init__(self,embed_size,num_heads):\n",
    "        super(transformer_block, self).__init__()\n",
    "\n",
    "        self.attention = nn.MultiheadAttention(embed_size, num_heads, batch_first=True)\n",
    "        self.fc = nn.Sequential(nn.Linear(embed_size, 4 * embed_size),\n",
    "                                nn.LeakyReLU(),\n",
    "                                nn.Linear(4 * embed_size, embed_size))\n",
    "        self.dropout = nn.Dropout(drop_prob)\n",
    "        self.ln1 = nn.LayerNorm(embed_size, eps=1e-6)\n",
    "        self.ln2 = nn.LayerNorm(embed_size, eps=1e-6)\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        attn_out, _ = self.attention(x, x, x, need_weights=False)\n",
    "        x = x + self.dropout(attn_out)\n",
    "        x = self.ln1(x)\n",
    "\n",
    "        fc_out = self.fc(x)\n",
    "        x = x + self.dropout(fc_out)\n",
    "        x = self.ln2(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "class transformer_forecaster(nn.Module):\n",
    "\n",
    "    def __init__(self,embed_size,num_heads,num_blocks):\n",
    "        super(transformer_forecaster, self).__init__()\n",
    "\n",
    "        num_len = len(numeric_covariates)\n",
    "        self.embedding_cov = nn.ModuleList([nn.Embedding(n,embed_size-num_len) for n in categorical_covariates_num_embeddings])\n",
    "        self.embedding_static = nn.ModuleList([nn.Embedding(n,embed_size-num_len) for n in categorical_static_num_embeddings])\n",
    "\n",
    "        self.blocks = nn.ModuleList([transformer_block(embed_size,num_heads) for n in range(num_blocks)])\n",
    "\n",
    "        self.forecast_head = nn.Sequential(nn.Linear(embed_size, embed_size*2),\n",
    "                                        nn.LeakyReLU(),\n",
    "                                        nn.Dropout(drop_prob),\n",
    "                                        nn.Linear(embed_size*2, embed_size*4),\n",
    "                                        nn.LeakyReLU(),\n",
    "                                        nn.Linear(embed_size*4, forecast_length),\n",
    "                                        nn.ReLU())\n",
    "\n",
    "    def forward(self, x_numeric, x_category, x_static):\n",
    "\n",
    "        tmp_list = []\n",
    "        for i,embed_layer in enumerate(self.embedding_static):\n",
    "            tmp_list.append(embed_layer(x_static[:,i]))\n",
    "        categroical_static_embeddings = torch.stack(tmp_list).mean(dim=0).unsqueeze(1)\n",
    "\n",
    "        tmp_list = []\n",
    "        for i,embed_layer in enumerate(self.embedding_cov):\n",
    "            tmp_list.append(embed_layer(x_category[:,:,i]))\n",
    "        categroical_covariates_embeddings = torch.stack(tmp_list).mean(dim=0)\n",
    "        T = categroical_covariates_embeddings.shape[1]\n",
    "\n",
    "        embed_out = (categroical_covariates_embeddings + categroical_static_embeddings.repeat(1,T,1))/2\n",
    "        x = torch.concat((x_numeric,embed_out),dim=-1)\n",
    "\n",
    "        for block in self.blocks:\n",
    "            x = block(x)\n",
    "\n",
    "        x = x.mean(dim=1)\n",
    "        x = self.forecast_head(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RMSLELoss(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.mse = nn.MSELoss()\n",
    "\n",
    "    def forward(self, pred, actual):\n",
    "        return torch.sqrt(self.mse(torch.log(pred + 1), torch.log(actual + 1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epoch = 1000\n",
    "min_val_loss = 999\n",
    "\n",
    "num_blocks = 1\n",
    "embed_size = 500\n",
    "num_heads = 50\n",
    "batch_size = 128\n",
    "learning_rate = 3e-4\n",
    "time_shuffle = False\n",
    "drop_prob = 0.1\n",
    "\n",
    "model = transformer_forecaster(embed_size,num_heads,num_blocks).to(device)\n",
    "criterion = RMSLELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(),lr=learning_rate)\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=50, gamma=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_loader = data_loader(x_numeric_train_tensor, x_category_train_tensor, x_static_train_tensor, y_train_tensor, batch_size, time_shuffle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([14083, 1])"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train_tensor.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([128, 0, 7]) torch.Size([128, 0, 8]) torch.Size([128, 1]) torch.Size([128, 0])\n"
     ]
    }
   ],
   "source": [
    "for a,b,c,d in batch_loader:\n",
    "    print(a.shape, b.shape, c.shape, d.shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([128, 16]) torch.Size([128, 0])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ymai/anaconda3/envs/ML/lib/python3.8/site-packages/torch/nn/modules/loss.py:536: UserWarning: Using a target size (torch.Size([128, 0, 1])) that is different to the input size (torch.Size([128, 16])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "The size of tensor a (128) must match the size of tensor b (0) at non-singleton dimension 1",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[38], line 16\u001b[0m\n\u001b[1;32m     14\u001b[0m preds \u001b[38;5;241m=\u001b[39m model(x_numeric, x_category, x_static)\n\u001b[1;32m     15\u001b[0m \u001b[38;5;28mprint\u001b[39m(preds\u001b[38;5;241m.\u001b[39mshape, y\u001b[38;5;241m.\u001b[39mshape)\n\u001b[0;32m---> 16\u001b[0m loss \u001b[38;5;241m=\u001b[39m \u001b[43mcriterion\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpreds\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43munsqueeze\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     17\u001b[0m train_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mitem()\n\u001b[1;32m     18\u001b[0m counter \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n",
      "File \u001b[0;32m~/anaconda3/envs/ML/lib/python3.8/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "Cell \u001b[0;32mIn[29], line 8\u001b[0m, in \u001b[0;36mRMSLELoss.forward\u001b[0;34m(self, pred, actual)\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, pred, actual):\n\u001b[0;32m----> 8\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m torch\u001b[38;5;241m.\u001b[39msqrt(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmse\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlog\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpred\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlog\u001b[49m\u001b[43m(\u001b[49m\u001b[43mactual\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m)\n",
      "File \u001b[0;32m~/anaconda3/envs/ML/lib/python3.8/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/anaconda3/envs/ML/lib/python3.8/site-packages/torch/nn/modules/loss.py:536\u001b[0m, in \u001b[0;36mMSELoss.forward\u001b[0;34m(self, input, target)\u001b[0m\n\u001b[1;32m    535\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor, target: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 536\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmse_loss\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreduction\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreduction\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/ML/lib/python3.8/site-packages/torch/nn/functional.py:3294\u001b[0m, in \u001b[0;36mmse_loss\u001b[0;34m(input, target, size_average, reduce, reduction)\u001b[0m\n\u001b[1;32m   3291\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m size_average \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m reduce \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   3292\u001b[0m     reduction \u001b[38;5;241m=\u001b[39m _Reduction\u001b[38;5;241m.\u001b[39mlegacy_get_string(size_average, reduce)\n\u001b[0;32m-> 3294\u001b[0m expanded_input, expanded_target \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbroadcast_tensors\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3295\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_C\u001b[38;5;241m.\u001b[39m_nn\u001b[38;5;241m.\u001b[39mmse_loss(expanded_input, expanded_target, _Reduction\u001b[38;5;241m.\u001b[39mget_enum(reduction))\n",
      "File \u001b[0;32m~/anaconda3/envs/ML/lib/python3.8/site-packages/torch/functional.py:74\u001b[0m, in \u001b[0;36mbroadcast_tensors\u001b[0;34m(*tensors)\u001b[0m\n\u001b[1;32m     72\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function(tensors):\n\u001b[1;32m     73\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(broadcast_tensors, tensors, \u001b[38;5;241m*\u001b[39mtensors)\n\u001b[0;32m---> 74\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_VF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbroadcast_tensors\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: The size of tensor a (128) must match the size of tensor b (0) at non-singleton dimension 1"
     ]
    }
   ],
   "source": [
    "for epoch in range(num_epoch):\n",
    "\n",
    "    batch_loader = data_loader(x_numeric_train_tensor, x_category_train_tensor, x_static_train_tensor, y_train_tensor, batch_size, time_shuffle)\n",
    "    train_loss = 0\n",
    "    counter = 0\n",
    "\n",
    "    # print(y.shape)\n",
    "\n",
    "\n",
    "    model.train()\n",
    "    for x_numeric, x_category, x_static, y in batch_loader:\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        preds = model(x_numeric, x_category, x_static)\n",
    "        print(preds.shape, y.shape)\n",
    "        loss = criterion(preds, y.unsqueeze(-1))\n",
    "        train_loss += loss.item()\n",
    "        counter += 1\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    train_loss = train_loss/counter\n",
    "    print(f'Epoch {epoch} training loss: {train_loss}')\n",
    "\n",
    "    model.eval()\n",
    "    val_batches = val_loader(x_numeric_val_tensor, x_category_val_tensor, x_static_val_tensor, y_val_tensor, batch_size, num_val)\n",
    "    val_loss = 0\n",
    "    counter = 0\n",
    "    for x_numeric_val, x_category_val, x_static_val, y_val in val_batches:\n",
    "        with torch.no_grad():\n",
    "            preds = model(x_numeric_val,x_category_val,x_static_val)\n",
    "            loss = criterion(preds,y_val).item()\n",
    "        val_loss += loss\n",
    "        counter += 1\n",
    "    val_loss = val_loss/counter\n",
    "    print(f'Epoch {epoch} validation loss: {val_loss}')\n",
    "\n",
    "    if val_loss<min_val_loss:\n",
    "        print('saved...')\n",
    "        torch.save(model,r'./long_best.model')\n",
    "        min_val_loss = val_loss\n",
    "\n",
    "    scheduler.step()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ML",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
